{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkSGVSxZDrx8"
      },
      "source": [
        "Tips:\n",
        "\n",
        "* After installing software, restart the Python runtime using *Runtime -> Restart*.\n",
        "\n",
        "* Should you need to reset your environment to a clean state, you can use *Runtime -> Disconnect and delete runtime*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v5mm4amQRrm"
      },
      "source": [
        "# GDG Ukraine 2022: Intro to TensorFlow Decision Forests\n",
        "\n",
        "Welcome! Today, you'll gain hands-on experience training decision forests with TensorFlow. Tree-based models incuding random forests and gradient-boosted trees are  some of the most [popular](https://www.kaggle.com/kaggle-survey-2021) models used in [Kaggle](https://kaggle.com/) compeititions, and are a valuable tool to become familiar with, in addition to neural networks.\n",
        "\n",
        "This notebook contains a tutorial and quick and exercise to help you get started. You'll train a random forest on a tabular dataset that you load from a CSV file. This is a common pattern in practice. As an exercise, you'll train a gradient boosted tree.\n",
        "\n",
        "Okay, let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpXrSP3bLltB"
      },
      "source": [
        "# Random Forests"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üå≤üå≥üå≤üå≥üå≤üêøÔ∏èüêª"
      ],
      "metadata": {
        "id": "G8cL-suNMJmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Forests are a family of tree-based models including Random Forests and Gradient Boosted Trees. They are the best place to start when working with tabular data, and will often outperform (or provide a strong baseline) before you begin experimenting with neural networks.\n",
        "\n",
        "You will use TensorFlow to train each of these on a dataset you load from a CSV file. This is a common pattern in practice. Roughly, your code will look as follows:\n",
        "\n",
        "```\n",
        "import tensorflow_decision_forests as tfdf\n",
        "import pandas as pd\n",
        "  \n",
        "dataset = pd.read_csv(\"project/dataset.csv\")\n",
        "tf_dataset = tfdf.keras.pd_dataframe_to_tf_dataset(dataset, label=\"my_label\")\n",
        "\n",
        "model = tfdf.keras.RandomForestModel()\n",
        "model.fit(tf_dataset)\n",
        "  \n",
        "print(model.summary())\n",
        "```"
      ],
      "metadata": {
        "id": "Z4eo3rH_MKbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install software\n",
        "\n",
        "There are many excellent libraries for working with tree-based models, including [scikit-learn](https://scikit-learn.org/) (highly recommended for all your ML needs), XGBoost, LightGBM, and others.\n",
        "\n",
        "Today, you'll use [TensorFlow Decision Forests (TF-DF)](https://www.tensorflow.org/decision_forests), a relatively new library used to train large models at Google. The open-source release is currently in beta. \n",
        "\n",
        "If you use TF-DF your work, we would love to hear about it. And/or, if you encounter bugs or friction not mentioned in the release notes, please email Josh."
      ],
      "metadata": {
        "id": "yjIk3fktvokL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sfVlsJENfzS"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow_decision_forests --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import the library"
      ],
      "metadata": {
        "id": "FVOXAyXl3-fA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may see a warnings about certain distrubuted training modes not being available during the beta. That's expected, and you can safely ignore these."
      ],
      "metadata": {
        "id": "8pk0VHfqnc22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_decision_forests as tfdf\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "IGmyjJJatzBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh4qwB4iN7Ue"
      },
      "outputs": [],
      "source": [
        "print(\"TensorFlow v\" + tf.__version__)\n",
        "print(\"TensorFlow Decision Forests v\" + tfdf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the penguins dataset\n",
        "\n",
        "To start, you will work with a small tabular [dataset](https://allisonhorst.github.io/palmerpenguins/articles/intro.html) of about 300 penguins. You will predict the species of penguin (Adelie, Gentoo, or Chinstrap) based on numeric attributes like their flipper length, and categorical attributes like the name of the island they're found on."
      ],
      "metadata": {
        "id": "-3vxMmCPvqpf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVMPH_IDOBH2"
      },
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "!wget -q https://storage.googleapis.com/download.tensorflow.org/data/palmer_penguins/penguins.csv -O /tmp/penguins.csv\n",
        "\n",
        "# Load a dataset into a Pandas Dataframe\n",
        "dataset_df = pd.read_csv(\"/tmp/penguins.csv\")\n",
        "\n",
        "# Display the first 3 examples\n",
        "dataset_df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the dataset"
      ],
      "metadata": {
        "id": "H4O7QCoh5e2e"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-ftBmKgRsZB"
      },
      "source": [
        "This dataset contains a mix of numeric (*bill_depth_mm*), categorical (*island*) and missing features. TF-DF supports all these feature types natively, and no preprocessing is required. This is one of the advantages of tree-based models, and why they're a great place to start.\n",
        "\n",
        "You will have to slightly adjusted the labels, though, to convert them into the integer format TF-DF expects. The label (species) is stored as a string, so let's convert that into an integer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9r3n7NOVRlx5"
      },
      "outputs": [],
      "source": [
        "label = \"species\"\n",
        "\n",
        "classes = dataset_df[label].unique().tolist()\n",
        "print(f\"Label classes: {classes}\")\n",
        "\n",
        "dataset_df[label] = dataset_df[label].map(classes.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brbRsBQfSC74"
      },
      "source": [
        "Next, split the dataset into training and testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsQad0t7SBv2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def split_dataset(dataset, test_ratio=0.30):\n",
        "  test_indices = np.random.rand(len(dataset)) < test_ratio\n",
        "  return dataset[~test_indices], dataset[test_indices]\n",
        "\n",
        "train_ds_pd, test_ds_pd = split_dataset(dataset_df)\n",
        "print(\"{} examples in training, {} examples in testing.\".format(\n",
        "    len(train_ds_pd), len(test_ds_pd)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hNGPbLlSGvp"
      },
      "source": [
        "There's one more step required before you can train your model. You need to convert from Pandas format (`pd.DataFrame`) into TensorFlow format (`tf.data.Dataset`). We've provided a single line helper function that will do this for you: \n",
        "\n",
        "```\n",
        "tfdf.keras.pd_dataframe_to_tf_dataset(your_df, label='species')\n",
        "```\n",
        "\n",
        "This is a high [performance](https://www.tensorflow.org/guide/data_performance) data loading library which is helpful when training neural networks with accelerators like GPUs and TPUs. It it not necessary for tree-based models until you begin to do distributed training - but we'll use it today for practice.\n",
        "\n",
        "Creating a fast input pipeline is important when working with neural networks, and forgetting to do so is the most common bug new researchers encounter. The author of this notebook has seen many folks with expensive GPUs that are idle ~50% of the time while waiting for data.\n",
        "\n",
        "Note that tf.data is a bit tricky to use, and has a learning curve. There are guides on [tensorflow.org/guide](https://www.tensorflow.org/guide) to help."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQgimfirSGQ9"
      },
      "outputs": [],
      "source": [
        "train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=label)\n",
        "test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_ds_pd, label=label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUG4UKUyTNUu"
      },
      "source": [
        "## What models are available?\n",
        "\n",
        "There are several tree-based models for you to choose from. To start, you'll work with a Random Forest. Thus is the most well-known of the Decision Forest training algorithms. \n",
        "\n",
        "A Random Forest is a collection of decision trees, each trained independently on a random subset of the training dataset (sampled with replacement). The algorithm is unique in that it is robust to overfitting, and easy to use."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfdf.keras.get_all_models()"
      ],
      "metadata": {
        "id": "MFmnkRR_Ui9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike neural networks, decision forests have relatively few (and easy to configure) hyperparameters with good defaults."
      ],
      "metadata": {
        "id": "ddQP-nGY9Qeu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How can I configure them?\n",
        "\n",
        "TF-DF provides good defaults for you (e.g. the top ranking hyperparameters on our benchmarks, slightly modified to run in reasonable time). You will use these defaults below. If you would like to configure the learning algorithm, you will find many options you can explore to get the highest possible accuracy. \n",
        "\n",
        "Let's check out the help on the ```RandomForestModel``` to see the options."
      ],
      "metadata": {
        "id": "LiFn716FnMVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(tfdf.keras.RandomForestModel)"
      ],
      "metadata": {
        "id": "BscU-0B3nGU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are **many** hyperparamters you can explore to grow exactly the type of forest you like. \n"
      ],
      "metadata": {
        "id": "GXLABIGERDEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can the parameters as follows\n",
        "print(tfdf.keras.RandomForestModel.predefined_hyperparameters())"
      ],
      "metadata": {
        "id": "Hr8k4TE8RQ62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can select a template and/or set parameters as follows:\n",
        "\n",
        "```gbt = tfdf.keras.GradientBoostedTreesModel(hyperparameter_template=\"benchmark_rank1\",num_trees=300)```\n"
      ],
      "metadata": {
        "id": "irxAS91IRVAX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Random Forest \n",
        "\n",
        "Today, you will use the defaults. Let's create your model. "
      ],
      "metadata": {
        "id": "AUt4j8fLWRlR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7bqOQMYTRXZ"
      },
      "outputs": [],
      "source": [
        "rf = tfdf.keras.RandomForestModel()\n",
        "rf.compile(metrics=[\"accuracy\"]) # Optional, you can use this to include a list of eval metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train your model\n",
        "\n",
        "This is a one-liner.\n",
        "\n",
        "Note: you may see a warning about Autograph. You can safely ignore this, it will be fixed in the next release."
      ],
      "metadata": {
        "id": "0CzJ5_sh91Yt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf.fit(x=train_ds)"
      ],
      "metadata": {
        "id": "Ax6RircN92LW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1HJ6KxRT7IR"
      },
      "source": [
        "## Visualize your model\n",
        "One benefit of tree-based models is that you can easily visualize them. The default number of trees used in the Random Forest is 300. You can select a tree to display below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTx73NgET9f8"
      },
      "outputs": [],
      "source": [
        "tfdf.model_plotter.plot_model_in_colab(rf, tree_idx=0, max_depth=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fazbJOgUT1n4"
      },
      "source": [
        "## Evaluate the model on OOB data and the test dataset\n",
        "\n",
        "Let's plot accuracy on OOB evaluation dataset as a function of the number of trees in the forest. One of the nice features about this particular hyperparameter is that larger values are usually better, and come with little risk aside from slowing down training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryddKoqLWrTp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "logs = rf.make_inspector().training_logs()\n",
        "plt.plot([log.num_trees for log in logs], [log.evaluation.accuracy for log in logs])\n",
        "plt.xlabel(\"Number of trees\")\n",
        "plt.ylabel(\"Accuracy (out-of-bag)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also see some general stats on the OOB dataset:"
      ],
      "metadata": {
        "id": "Y-yMMsK5-3Mr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdY8DvriTxky"
      },
      "outputs": [],
      "source": [
        "inspector = rf.make_inspector()\n",
        "inspector.evaluation()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's run an evaluation using the test data. Depending on the random split your accuracy will likely between 90-100%."
      ],
      "metadata": {
        "id": "GAoGJNjg-9sb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39x97YqWZlgm"
      },
      "outputs": [],
      "source": [
        "evaluation = rf.evaluate(x=test_ds,return_dict=True)\n",
        "\n",
        "for name, value in evaluation.items():\n",
        "  print(f\"{name}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variable importances\n",
        "\n",
        "There are several ways to identify important features. Let's see the available options:"
      ],
      "metadata": {
        "id": "LWWqqDLM7WdZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xok16_jMgGZH"
      },
      "outputs": [],
      "source": [
        "print(f\"Available variable importances:\")\n",
        "for importance in inspector.variable_importances().keys():\n",
        "  print(\"\\t\", importance)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's display one of them:"
      ],
      "metadata": {
        "id": "USvNgqBR_JR2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eI073gJHgHxr"
      },
      "outputs": [],
      "source": [
        "inspector.variable_importances()[\"NUM_AS_ROOT\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T75nMlpRc7mz"
      },
      "source": [
        "## Predict on a single example\n",
        "\n",
        "Here's example code you can use to make predictions on a single example. Note that TensorFlow is optimized for batch prediction. This code below is mainly helpful for experimenting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLXxNfsXYPS7"
      },
      "outputs": [],
      "source": [
        "# Create your example as a dictionary\n",
        "example = {\"bill_depth_mm\" : [0],\n",
        "           \"bill_length_mm\" : [0],\n",
        "           \"body_mass_g\" : [0],\n",
        "           \"flipper_length_mm\" : [0],\n",
        "           \"island\" : [\"Torgersen\"],\n",
        "           \"sex\" : \"female\",\n",
        "           \"year\" : 2007}\n",
        "\n",
        "# Convert the dictionary into a DataFrame\n",
        "example_df = pd.DataFrame.from_dict(example)\n",
        "\n",
        "# Convert the DataFrame into tf.data format\n",
        "example_ds = tfdf.keras.pd_dataframe_to_tf_dataset(example_df)\n",
        "\n",
        "# Call predict\n",
        "rf.predict(example_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YRjnYPwc1lj"
      },
      "source": [
        "## Predict on many examples\n",
        "\n",
        "Following is code you can use to display predictions for each example in the test set. Note that similar code will be a bit different for neural networks, which typically use a different data structure inside tf.data to pack the features and labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhaqythabYI9"
      },
      "outputs": [],
      "source": [
        "# Make predictions on every example in the test set\n",
        "predictions = rf.predict(test_ds)\n",
        "\n",
        "# Loop over the test set, and display the predicted value and label\n",
        "features, labels = next(iter(test_ds))\n",
        "for pred, label in zip(predictions, labels):\n",
        "  print (\"Pred:\", np.argmax(pred), \"Actual:\", label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: Gradient Boosted Trees\n",
        "\n",
        "In this exercise you will download the [census](https://archive.ics.uci.edu/ml/datasets/census+income) dataset which. This contains ~40K examples with a mix of numeric and categorical attributes. You will train a gradient boosted tree, identify important features, and evaluate your model's accuracy.\n",
        "\n",
        "We've provided a bunch of code you can use to explore the dataset, in case this is helpful to you in your future work. The code you need to write for this exercise is only a couple lines.\n",
        "\n",
        "Notes:\n",
        "- You can visualize this dataset in your browser using https://pair-code.github.io/facets/ \n",
        "- This dataset has fairness concerns, which you can learn about at https://www.tensorflow.org/responsible_ai\n",
        "\n",
        "### Instructions\n",
        "\n",
        "Complete the code cells below. See the comments for instructions. You can find a solution at the end."
      ],
      "metadata": {
        "id": "h2-4vFn2kSCP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download and explore the dataset"
      ],
      "metadata": {
        "id": "oHedYD3YCaXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "!wget https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/census/adult.data -O /tmp/adult.csv"
      ],
      "metadata": {
        "id": "4NRWwxOjXMs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the CSV\n",
        "!head /tmp/adult.csv"
      ],
      "metadata": {
        "id": "nanlkRImZQJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The CSV is missing a header\n",
        "cols = [\n",
        "    'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
        "    'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
        "    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'label'\n",
        "]\n",
        "\n",
        "df = pd.read_csv(\"/tmp/adult.csv\", names=cols)"
      ],
      "metadata": {
        "id": "OCmYzMxO895q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up\n",
        "\n",
        "# Drop a meaningless column\n",
        "df = df.drop(columns=['fnlwgt'])\n",
        "\n",
        "# Convert the label into integer format\n",
        "df[\"label\"] = df[\"label\"].apply(lambda x: 1 if x == ' >50K' else 0).values\n",
        "\n",
        "# Shuffle the dataset\n",
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "m0qRrGpG3hHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info(verbose=True, show_counts=True)"
      ],
      "metadata": {
        "id": "M2aO04MpchgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_col = 'label'\n",
        "categorical_columns = list(df.select_dtypes(include='object').columns)\n",
        "numeric_columns = [c for c in df.columns if c not in categorical_columns]\n",
        "\n",
        "print('Categorical columns', categorical_columns)\n",
        "print('Numeric columns', numeric_columns)\n",
        "\n",
        "feature_columns = categorical_columns + numeric_columns"
      ],
      "metadata": {
        "id": "Fp8clnol4K0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = split_dataset(df)\n",
        "print(\"{} examples in training, {} examples in testing.\".format(\n",
        "    len(train_df), len(test_df)))"
      ],
      "metadata": {
        "id": "j5TFXHAf3xUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[numeric_columns].describe()"
      ],
      "metadata": {
        "id": "QQB9PvjJ4JX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[categorical_columns].nunique()"
      ],
      "metadata": {
        "id": "wAApEU-z4NGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in categorical_columns:\n",
        "  print(col, list(train_df[col].unique()))"
      ],
      "metadata": {
        "id": "3dioChz34a46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the class balance?"
      ],
      "metadata": {
        "id": "ZZEtOIgg7eA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"label\"].sum() / len(train_df)"
      ],
      "metadata": {
        "id": "o4kDNZk84fXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df[\"label\"].sum() / len(test_df)"
      ],
      "metadata": {
        "id": "ZF4RqLUF4fxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train shape:', train_df.shape)\n",
        "print('Test shape :', test_df.shape)"
      ],
      "metadata": {
        "id": "Nr3KLRCLYvs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create tf.data.Datasets from the Pandas DataFrame, using the one-liner shown above."
      ],
      "metadata": {
        "id": "90KkJPxNa7Jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "# Add code to create a tf.data.Dataset for train and test from the DataFrames\n",
        "\n",
        "# Example...\n",
        "# train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(...\n",
        "# test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(..."
      ],
      "metadata": {
        "id": "CVhwsA0zXTl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create and train your model"
      ],
      "metadata": {
        "id": "Vx5UwlUCCeh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create your model. You can write a one-liner for this, similar to the example above. Previously, you learned how to work with Random Forests. If you like, you can create a Random Forest again, or you can try creating a Gradient Boosted Tree. \n",
        "\n",
        "As a reminder, you can see which models are available by running `tfdf.keras.get_all_models()`, or visiting [tensorflow.org/decision_forests](https://www.tensorflow.org/decision_forests)."
      ],
      "metadata": {
        "id": "M4QE2L38bRzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "# Add code to create a gradient boosted tree\n",
        "# Example ...\n",
        "# gbt = tfdf.keras. ...\n",
        "# gbt.compile(metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "HfT7KrdFkJAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train your model. You can write a one-liner for this, similar to the example above."
      ],
      "metadata": {
        "id": "xmLQLPtxkPIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "# Add code to train your model\n",
        "# Example ...\n",
        "# gbt.fit(..."
      ],
      "metadata": {
        "id": "MQnw48g5kOgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate your model\n",
        "\n",
        "Uncomment these cells after completing the code above."
      ],
      "metadata": {
        "id": "8PJ7yVhxmxxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#gbt.summary()"
      ],
      "metadata": {
        "id": "pt9QZcxNtJr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gbt.evaluate(test_ds)"
      ],
      "metadata": {
        "id": "EoDaNgB6m2OM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation = gbt.evaluate(x=test_ds,return_dict=True)\n",
        "\n",
        "# for name, value in evaluation.items():\n",
        "#   print(f\"{name}: {value:.4f}\")"
      ],
      "metadata": {
        "id": "VKcodwmWtqxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspector = gbt.make_inspector()\n",
        "# inspector.evaluation()"
      ],
      "metadata": {
        "id": "nPMWQUQn5I7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspector.variable_importances()[\"NUM_AS_ROOT\"]"
      ],
      "metadata": {
        "id": "radLWEm75Ke5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tfdf.model_plotter.plot_model_in_colab(gbt, tree_idx=0, max_depth=3)"
      ],
      "metadata": {
        "id": "JL-ChwmK5R0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution\n",
        "\n",
        "\n",
        "**Create tf.data.Datasets from the pd.DataFrame**\n",
        "\n",
        "To do so, you can write:\n",
        "\n",
        "```\n",
        "train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_df, label=\"label\")\n",
        "test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_df, label=\"label\")\n",
        "```\n",
        "\n",
        "If you will be working with the same tf.data.Dataset multiple times, you can add `.cache()` at the end of those lines to keep it in memory.\n",
        "\n",
        "**Create and train a GradientBoostedTrees model**\n",
        "\n",
        "To do so, you can write:\n",
        "\n",
        "```\n",
        "gbt = tfdf.keras.GradientBoostedTreesModel()\n",
        "gbt.compile(metrics=[\"accuracy\"])\n",
        "gbt.fit(train_ds)\n",
        "```\n"
      ],
      "metadata": {
        "id": "oHRGTK6qksZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next steps\n",
        "\n",
        "**Try a larger dataset**\n",
        "\n",
        "Thanks to our friends at Kaggle, you can find a tabular dataset with ~1.7M rows and starter code for TensorFlow Decision Forests [here](https://www.kaggle.com/code/paultimothymooney/getting-started-with-tensorflow-decision-forests/). This is great if you'd like to start running larger experiments.\n",
        "\n",
        "**Hyperparameter tuning**\n",
        "\n",
        "You can use [Keras Tuner](https://keras.io/keras_tuner/) for easy hyperparameter optmization. "
      ],
      "metadata": {
        "id": "LoJev14jkb9k"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "tfdf-example.ipynb",
      "provenance": [],
      "toc_visible": true,
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}